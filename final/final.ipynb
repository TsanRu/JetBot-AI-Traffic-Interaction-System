{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project05\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pycuda.autoinit\n",
    "from utils.yolo_classes import get_cls_dict\n",
    "from utils.display import open_window, set_display, show_fps\n",
    "from utils.visualization import BBoxVisualization\n",
    "from utils.yolo import TRT_YOLO\n",
    "\n",
    "trt_yolo = TRT_YOLO(\"yolov4-tiny-416\", (416, 416), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midterm \n",
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('best_steering_model_xy_trt.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midterm\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midterm\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midterm\n",
    "image_widget = ipywidgets.Image()\n",
    "\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.12,description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.04, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.25, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Response & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def openai(image_path):\n",
    "    url = \"http://192.168.99.79:8000/get_openai\"    \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        files = {\"file\": image_file}\n",
    "        response = requests.post(url, files=files)\n",
    "    \n",
    "    # Handle the response\n",
    "    if response.status_code == 200:\n",
    "        # print(\"Response content:\", response.json())\n",
    "        print(response.json())\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        \n",
    "def openai_text(sign):\n",
    "    url = \"http://192.168.99.79:8000/get_openai_text\"    \n",
    "    response = requests.post(url, data=sign)\n",
    "    \n",
    "    # Handle the response\n",
    "    if response.status_code == 200:\n",
    "        # print(\"Response content:\", response.json())\n",
    "        print(response.json())\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_test = cv2.imread(\"./detected_img/road_close_2024-12-27 07:00:46.jpeg\")\n",
    "response = openai(\"./detected_img/road_close.jpeg\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total memory: {memory.total / (1024**3):.2f} GB\") # 總記憶體\n",
    "print(f\"Available memory: {memory.available / (1024**3):.2f} GB\") # 可用記憶體\n",
    "print(f\"Used memory: {memory.used / (1024**3):.2f} GB\") # 已使用記憶體"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo Object Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_t = 0\n",
    "sign_class={0:\"道路封閉\", 1:\"減速行駛\", 2:\"加速行駛\", 3:\"行人(pedestrian)\", 4:\"鐵路平交道\", 5:\"停車再開(stop)\"}\n",
    "\n",
    "def yolo_test(change):\n",
    "    global pre_t\n",
    "    image = change['new']\n",
    "    image_resize = cv2.resize(image, (416, 416))\n",
    "    img = image_resize.copy() \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "#     print(\"image: \",image.shape)\n",
    "#     print(\"image_resize: \",image_resize.shape)\n",
    "    start = time.time()\n",
    "    boxes, confs, clss = trt_yolo.detect(image_resize)\n",
    "    print(f\"class: {sign_class[int(clss[0])]}\")\n",
    "#     print(\"yolo 耗時：\", time.time()-start)\n",
    "#     print(\"boxes: \",boxes)\n",
    "    cv2.imwrite(\"./detected_img/test_origin.jpeg\",image)\n",
    "    if len(boxes)!=0:\n",
    "        for i in range(len(clss)):\n",
    "            cv2.rectangle(img, (boxes[i][0], boxes[i][1]), (boxes[i][2], boxes[i][3]), (0, 0, 255), 1) # wen\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.imwrite(\"./detected_img/test.jpeg\",img)\n",
    "        start = time.time()\n",
    "#         openai(\"./detected_img/test.jpeg\")\n",
    "        print(\"openai 耗時：\", time.time()-start)\n",
    "\n",
    "#     print(\"-- end --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_test({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section of Sign Detection (Thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global frame Queue\n",
    "import threading\n",
    "from queue import Queue\n",
    "frame_queue = Queue(maxsize=1)\n",
    "pre_t = 0\n",
    "speed_factor = 1\n",
    "steering_factor = 1\n",
    "sign_class={0:\"道路封閉\", 1:\"減速行駛(速限60)\", 2:\"加速行駛(速限30)\", 3:\"行人(pedestrian)\", 4:\"鐵路平交道\", 5:\"停車再開(stop)\"}\n",
    "sign_filename={0:\"block\", 1:\"max60\", 2:\"min30\", 3:\"pedestrain\", 4:\"railway\", 5:\"stop\"}\n",
    "used_sign = []\n",
    "\n",
    "def yolo():\n",
    "    global pre_t, speed_factor, steering_factor, used_sign\n",
    "    ALERT_WIDTH = 40 \n",
    "    while(1):\n",
    "        image = frame_queue.get()\n",
    "        image_resize = cv2.resize(image, (416, 416))\n",
    "        img = image_resize.copy() \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        start = time.time()\n",
    "        boxes, confs, clss = trt_yolo.detect(image_resize)\n",
    "#         print(\"yolo 耗時：\", time.time()-start)\n",
    "        cv2.imwrite(\"./detected_img/test_origin.jpeg\",image)\n",
    "        t = time.time()\n",
    "        if len(boxes)!=0:\n",
    "            width = boxes[0][2] - boxes[0][0]\n",
    "        else:\n",
    "            width = 0\n",
    "            \n",
    "        if len(boxes)!=0 and t > pre_t + 5 and width > ALERT_WIDTH and (int(clss[0])not in used_sign):\n",
    "            used_sign.append(int(clss[0]))\n",
    "            print(f\"第{t}秒 class: {sign_class[int(clss[0])]}\")\n",
    "            \n",
    "#             0 道路封閉\n",
    "#             1 減速行駛\n",
    "#             2 加速行駛\n",
    "#             3 行人(pedestrian)\n",
    "#             4 鐵路平交道\n",
    "#             5 停車再開(stop)\n",
    "            \n",
    "            if int(clss[0]) == 0:\n",
    "                speed_factor = 0\n",
    "                steer_factor = 0\n",
    "            \n",
    "            elif int(clss[0]) in [2, 3]:\n",
    "                speed_factor = 0.9\n",
    "                \n",
    "            elif int(clss[0]) == 1:\n",
    "                speed_factor = 1.2\n",
    "                \n",
    "            elif int(clss[0]) == 4:\n",
    "                speed_factor = 0\n",
    "                steering_factor = 0\n",
    "                time.sleep(5)\n",
    "                speed_factor = 1\n",
    "                steering_factor = 1\n",
    "                \n",
    "            elif int(clss[0]) == 5:\n",
    "                speed_factor = 0\n",
    "                steering_factor = 0\n",
    "                time.sleep(2)\n",
    "                speed_factor = 1\n",
    "                steering_factor =1\n",
    "\n",
    "            for i in range(len(clss)):\n",
    "                cv2.rectangle(img, (boxes[i][0], boxes[i][1]), (boxes[i][2], boxes[i][3]), (0, 0, 255), 1) # wen\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            cv2.imwrite(\"./detected_img/test.jpeg\",img)\n",
    "            start = time.time()\n",
    "            pre_t = time.time() # 更新時間\n",
    "#             print(\"輸入照片：\"+\"./standard_sign/\"+sign_filename[int(clss[0])]+\".jpeg\")\n",
    "            openai(\"./standard_sign/\"+sign_filename[int(clss[0])]+\".jpeg\")\n",
    "            print(\"-- openai 耗時：\", time.time()-start,\"--\")\n",
    "#             pre_t = time.time() # 更新時間\n",
    "            \n",
    "        if image is None:\n",
    "            print(\"kill thread\")\n",
    "            frame_queue.task_done()\n",
    "            break\n",
    "#     print(\"-- end --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section of Path Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midterm\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last #midterm\n",
    "    global robot, count, speed_left, speed_right, stop_ignore, railway_ignore, pedestrian_ignore # project5\n",
    "    global speed_factor, steering_factor\n",
    "    ALERT_WIDTH = 40 # project5\n",
    "    \n",
    "    image = change['new']\n",
    "\n",
    "    ######################## midterm #############################\n",
    "    \n",
    "    xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value * speed_factor\n",
    "    \n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "\n",
    "    steering_slider.value = (pid + steering_bias_slider.value) * steering_factor\n",
    "\n",
    "    \n",
    "    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "\n",
    "    \n",
    "    \n",
    "    ####################### project5 #############################\n",
    "#     yolo(image)\n",
    "    if not frame_queue.full():\n",
    "#         print(\"put img\")\n",
    "        frame_queue.put(image)\n",
    "    \n",
    "\n",
    "sign_detection = threading.Thread(target=yolo)\n",
    "sign_detection.start()\n",
    "\n",
    "pre_t = time.time()\n",
    "# execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Whole Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_t = time.time()\n",
    "used_sign = []\n",
    "speed_factor = steering_factor = 1\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Execution & Release Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
